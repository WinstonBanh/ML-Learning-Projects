{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "593c86fa",
   "metadata": {},
   "source": [
    "To process text files, I use the Natural Language Toolkil (NLTK). There are some much more powerful toolkits that can instantly process text files without coding step by step. However, for the learning purpose, I will use NLTK to help myself farmilize with the concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de0ca934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/winston/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/winston/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/winston/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/winston/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/winston/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d58b2fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5674a5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disable warning in Anaconda\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fed834",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2b64cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['content','class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04178ae2",
   "metadata": {},
   "source": [
    "In reality, the text files are in various encodings. To detect the files' encoding and convert it into a Unicode string format so, we use the UnicodeDammit from bs4 lib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3fed500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import UnicodeDammit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb3b1cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Data/bbc/'\n",
    "for directory in os.listdir(path):\n",
    "    directory = os.path.join(path, directory)\n",
    "    if os.path.isdir(directory):\n",
    "        for filename in os.listdir(directory):\n",
    "            filename = os.path.join(directory, filename)\n",
    "            encoding = ''\n",
    "            #Using UnicodeDammit to detect and suggest the suitable encoding for the text data\n",
    "            with open(filename,'rb') as f: #UnicodeDammit read the file in binary mode ('rb')\n",
    "                content = f.read()\n",
    "                suggestion = UnicodeDammit(content)\n",
    "                encoding = suggestion.original_encoding\n",
    "                \n",
    "            #From the suggestion of encoding, using that encoding to read the file and append into the dataframe \n",
    "            with open(filename, encoding=encoding) as f:\n",
    "                content = f.read()\n",
    "                current_df = pd.DataFrame({'content':[content],'class':[os.path.basename(directory)]})\n",
    "                df = df.append(current_df, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a16af1c",
   "metadata": {},
   "source": [
    "Recheck the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caf9f168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2225 entries, 0 to 2224\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   content  2225 non-null   object\n",
      " 1   class    2225 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 34.9+ KB\n"
     ]
    }
   ],
   "source": [
    "#Overview the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23e29f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "content    0\n",
       "class      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check missing data\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77efee74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f1b4538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop duplicates and check again\n",
    "df.drop_duplicates(inplace = True)\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c8d72af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Musicians to tackle US red tape\\n\\nMusicians' ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U2's desire to be number one\\n\\nU2, who have w...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rocker Doherty in on-stage fight\\n\\nRock singe...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Snicket tops US box office chart\\n\\nThe film a...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ocean's Twelve raids box office\\n\\nOcean's Twe...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content          class\n",
       "0  Musicians to tackle US red tape\\n\\nMusicians' ...  entertainment\n",
       "1  U2's desire to be number one\\n\\nU2, who have w...  entertainment\n",
       "2  Rocker Doherty in on-stage fight\\n\\nRock singe...  entertainment\n",
       "3  Snicket tops US box office chart\\n\\nThe film a...  entertainment\n",
       "4  Ocean's Twelve raids box office\\n\\nOcean's Twe...  entertainment"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5c5d3d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>Warning over Windows Word files\\n\\nWriting a M...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>Fast lifts rise into record books\\n\\nTwo high-...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>Nintendo adds media playing to DS\\n\\nNintendo ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>Fast moving phone viruses appear\\n\\nSecurity f...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>Hacker threat to Apple's iTunes\\n\\nUsers of Ap...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content class\n",
       "2220  Warning over Windows Word files\\n\\nWriting a M...  tech\n",
       "2221  Fast lifts rise into record books\\n\\nTwo high-...  tech\n",
       "2222  Nintendo adds media playing to DS\\n\\nNintendo ...  tech\n",
       "2223  Fast moving phone viruses appear\\n\\nSecurity f...  tech\n",
       "2224  Hacker threat to Apple's iTunes\\n\\nUsers of Ap...  tech"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67561be",
   "metadata": {},
   "source": [
    "#### Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "120588dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove HTTP links\n",
    "df['content'] = df['content'].replace(r'((https|http)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*','',regex = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22929ff",
   "metadata": {},
   "source": [
    "Regex for http or https: ((https|http)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b1a538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove end of line characters\n",
    "df['content'] = df['content'].replace(r'[\\r\\n]+',' ',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe6394a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove numbers, only keep letters\n",
    "df['content'] = df['content'].replace('[\\w]*\\d+[\\w]*','',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce0086c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuation\n",
    "df['content'] = df['content'].replace('[^\\w\\s]',' ',regex=True)\n",
    "punctuation = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "for char in punctuation:\n",
    "    df['content'] = df['content'].replace(char,' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f231cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove multiple spaces with one space\n",
    "df['content'] = df['content'].replace('[\\s]{2,}',' ',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da049206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to lower case\n",
    "df['content'] = df['content'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab686a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Triming the spaces at the start and end of the lines\n",
    "df['content'] = df['content'].replace('^[\\s]{1,}','',regex=True)\n",
    "df['content'] = df['content'].replace('[\\s]{1,}$','',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b07e7d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove empty rows\n",
    "df = df[df['content'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8e8fff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text_split = text.split()\n",
    "    text = [word for word in text_split if word not in stop_words]\n",
    "    return ' '.join(text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89b88875",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9ff44c",
   "metadata": {},
   "source": [
    "Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab69003b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use word net lemmatizer to get the root of the word\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38ab0bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to identify verb, noun, adj...\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''\n",
    "    return WORDNET POS compliance to WORDNET lemmatization (a,n,r,v)\n",
    "    '''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    else:\n",
    "        #As defaul pos in lemmatization is Noun\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7071c8fd",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing inflection in words to their root forms. For example, 'consult', 'consultant', 'consulting', 'consultantative' etc will return to 'consult' after being stemming.\n",
    "\n",
    "However, 'studying' and 'study' will become 'studi', while 'change' and 'changing' become 'chang'.\n",
    "\n",
    "Thus, lemmatization method, similar but more powerful and more accurate will be preferred. Lemmatization is also the process of converting a word to its base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf684b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    lemmatized = []\n",
    "    post_tag_list = pos_tag(word_tokenize(text))\n",
    "    for word, post_tag_val in post_tag_list:\n",
    "        lemmatized.append(lemmatizer.lemmatize(word,get_wordnet_pos(post_tag_val)))\n",
    "    text = ' '.join(x for x in lemmatized)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f4b0b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09514e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the processed text data into excel\n",
    "df.to_excel('bbc_cleaned.xlsx',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
